{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aab3ddc-d58c-496b-8b06-b46bfd9c3892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# landing_path = \"/Volumes/workspace/source_data/landing\"\n",
    "# schema = \"workspace.bronze\"\n",
    "\n",
    "# for file_name in os.listdir(landing_path):\n",
    "#     file_path = os.path.join(landing_path, file_name)\n",
    "#     if os.path.isfile(file_path):\n",
    "#         table_name = os.path.splitext(file_name)[0].replace(\"-\", \"_\").replace(\".\", \"_\")\n",
    "#         df = spark.read.option(\"header\", \"true\").csv(file_path)\n",
    "#         df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{schema}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2a19eee-171c-407a-9706-e6ab8b06c135",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = \"workspace.bronze\"\n",
    "source_details_df = spark.table(\"workspace.control.source_file_details\").filter(\"is_active = true\")\n",
    "source_specs_df = spark.table(\"workspace.control.source_file_specifications\").filter(\"is_active = true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61ae5253-b6dd-4d52-95d2-1f8003c5b547",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Read from source_file_details and join with source_file_specifications table, to validate schema\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e082434f-5f3e-4ce9-8471-31b10bf6576c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for row in source_details_df.collect():\n",
    "    source_id = row['source_id']\n",
    "    file_path = row['file_path']\n",
    "    file_format = row['file_format']\n",
    "    file_delimitor = row['file_delimitor']\n",
    "    is_header = row['is_header']\n",
    "    file_name = row['file_name']\n",
    "    load_type = row['load_type']\n",
    "    table_name = file_name.replace(\"-\", \"_\").replace(\".\", \"_\")\n",
    "    \n",
    "    # display(source_id)\n",
    "    # Get field specifications for this source_id\n",
    "    spec_fields = (\n",
    "        source_specs_df\n",
    "        .filter(f\"source_id = '{source_id}'\")\n",
    "        .orderBy(\"field_order\")\n",
    "        .select(\"field_name\", \"data_type\")\n",
    "        .collect()\n",
    "    )\n",
    "\n",
    "    # display(spec_fields)\n",
    "    field_names = [f['field_name'] for f in spec_fields]\n",
    "    field_types = [f['data_type'] for f in spec_fields]\n",
    "    \n",
    "    # Build schema string for Spark\n",
    "    schema_str = \", \".join([f\"{n} {t}\" for n, t in zip(field_names, field_types)])\n",
    "    \n",
    "    # Read file with or without header\n",
    "    read_options = {\n",
    "        \"delimiter\": file_delimitor,\n",
    "        \"inferSchema\": \"false\",\n",
    "        \"header\": \"true\" if is_header else \"false\"\n",
    "    }\n",
    "    df = spark.read.format(file_format).options(**read_options).load(file_path)\n",
    "    \n",
    "    if is_header:\n",
    "        # Validate schema: header columns must match spec field_names\n",
    "        df_cols = [c.lower() for c in df.columns]\n",
    "        spec_cols = [c.lower() for c in field_names]\n",
    "        if df_cols != spec_cols:\n",
    "            raise Exception(f\"Schema mismatch for {file_name}: file columns {df.columns} != spec {field_names}\")\n",
    "        # Select columns in spec order\n",
    "        df = df.select(field_names)\n",
    "    else:\n",
    "        # No header: assign columns from spec\n",
    "        df = df.toDF(*field_names)\n",
    "    \n",
    "    # Cast columns to specified data types\n",
    "    for name, dtype in zip(field_names, field_types):\n",
    "        df = df.withColumn(name, df[name].cast(dtype))\n",
    "    \n",
    "    # Write to Delta table\n",
    "    write_mode = \"overwrite\" if load_type.lower() == \"full_load\" else \"append\"\n",
    "    df.write.format(\"delta\").mode(write_mode).saveAsTable(f\"{schema}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ir_orchestration_source_to_raw",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
