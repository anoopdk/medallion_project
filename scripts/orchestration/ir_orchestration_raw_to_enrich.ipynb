{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ecb3694d-f2b0-4f69-ac34-cbee38d8f1b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2aab3ddc-d58c-496b-8b06-b46bfd9c3892",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"table_name\",\"investor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dac47e80-14b0-4b64-99d4-751cfb6f0c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "table_name = dbutils.widgets.get(\"table_name\")\n",
    "display(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "518ea829-61f9-4857-8573-2835b931053a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, trim, regexp_replace, when, to_date, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41ec5b95-e5ab-458a-b868-742012d5d152",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def load_bronze_table(spark, table_name):\n",
    "    return spark.table(f\"workspace.bronze.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3c65420-791a-4842-ae19-949123658c6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_not_null(df, pk_cols):\n",
    "    for pk in pk_cols:\n",
    "        if df.filter(col(pk).isNull()).count() > 0:\n",
    "            raise Exception(f\"Null value found in primary key column: {pk}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "819c3fed-4057-422b-ad9b-a751806307c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def check_no_duplicates(df, pk_cols):\n",
    "    dup_count = df.groupBy(pk_cols).count().filter(col(\"count\") > 1).count()\n",
    "    if dup_count > 0:\n",
    "        raise ValueError(\"Duplicate primary key values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5962948-9eda-46aa-b534-bc71dac4b3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_string_columns(df, string_cols):\n",
    "    for col_name in string_cols:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            regexp_replace(trim(col(col_name)), \"[$#%]\", \"\")\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffe06d52-f1c9-48a6-b50e-96e1666e10c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_numeric_columns(df, numeric_cols):\n",
    "    for col_name in numeric_cols:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            when(col(col_name).isNull(), lit(0)).otherwise(col(col_name))\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d924d99-980a-4abd-a74e-820d296df80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def clean_date_columns(df, date_cols):\n",
    "    for col_name in date_cols:\n",
    "        df = df.withColumn(\n",
    "            col_name,\n",
    "            when(\n",
    "                col(col_name).isNull(),\n",
    "                to_date(lit(\"9999-12-31\"), \"yyyy-MM-dd\")\n",
    "            ).otherwise(\n",
    "                to_date(col(col_name), \"yyyy-MM-dd\")\n",
    "            )\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f01ae4cd-06b4-40a7-8901-c38bf0c1fbe9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fill_na_with_default(df, na_fields):\n",
    "    for field, default_value, data_type in na_fields:\n",
    "        if data_type.lower() in [\"int\", \"integer\", \"bigint\", \"smallint\"]:\n",
    "            casted_default = lit(int(default_value)).cast(data_type)\n",
    "        elif data_type.lower() in [\"float\", \"double\", \"decimal\"]:\n",
    "            casted_default = lit(float(default_value)).cast(data_type)\n",
    "        elif data_type.lower() in [\"date\"]:\n",
    "            casted_default = to_date(lit(default_value), \"yyyy-MM-dd\")\n",
    "        elif data_type.lower() in [\"timestamp\"]:\n",
    "            casted_default = to_timestamp(lit(default_value))\n",
    "        else:\n",
    "            casted_default = lit(default_value).cast(data_type)\n",
    "        df = df.withColumn(\n",
    "            field,\n",
    "            when(col(field).isNull(), casted_default).otherwise(col(field))\n",
    "        )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8772028b-d1c2-4b16-85cb-f03c65ca2015",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def rename_columns_from_constraints(spark, table_name, schema=\"workspace.silver\"):\n",
    "    constraints_df = (\n",
    "        spark.table(\"workspace.control.enrich_table_constraints\")\n",
    "        .filter(\n",
    "            (col(\"table_name\") == table_name) &\n",
    "            (col(\"is_active\") == True) &\n",
    "            (col(\"field_rename\").isNotNull()) &\n",
    "            (col(\"field_rename\") != \"na\")\n",
    "        )\n",
    "        .select(\"field_name\", \"field_rename\")\n",
    "        .distinct()\n",
    "    )\n",
    "    rename_pairs = [(row.field_name, row.field_rename) for row in constraints_df.collect()]\n",
    "    table_schema = spark.catalog.listColumns(f\"{schema}.{table_name}\")\n",
    "    existing_cols = {col.name for col in table_schema}\n",
    "    for old_col, new_col in rename_pairs:\n",
    "        if old_col != new_col and old_col in existing_cols and new_col not in existing_cols:\n",
    "            sql = f'ALTER TABLE {schema}.{table_name} RENAME COLUMN {old_col} TO {new_col}'\n",
    "            spark.sql(sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639e573e-d18e-430e-9214-0d85b2761def",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def incremental_upsert(spark, df, table_name, pk_fields):\n",
    "#     silver_table = f\"workspace.silver.{table_name}\"\n",
    "#     table_exists = spark.catalog.tableExists(silver_table)\n",
    "#     if not table_exists:\n",
    "#         df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "#     else:\n",
    "#         df.createOrReplaceTempView(\"staging_view\")\n",
    "#         merge_condition = \" AND \".join([f\"t.{pk}=s.{pk}\" for pk in pk_fields])\n",
    "#         set_clause = \", \".join([f\"t.{c}=s.{c}\" for c in df.columns])\n",
    "#         insert_cols = \", \".join(df.columns)\n",
    "#         insert_vals = \", \".join([f\"s.{c}\" for c in df.columns])\n",
    "#         merge_sql = f\"\"\"\n",
    "#             MERGE INTO {silver_table} AS t\n",
    "#             USING staging_view AS s\n",
    "#             ON {merge_condition}\n",
    "#             WHEN MATCHED THEN UPDATE SET {set_clause}\n",
    "#             WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\n",
    "#         \"\"\"\n",
    "#         spark.sql(merge_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1809eb1-6e66-4f3e-bba4-5bc3cbd9b0be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def incremental_upsert(spark, df, table_name, pk_fields):\n",
    "    from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "    silver_table = f\"workspace.silver.{table_name}\"\n",
    "    table_exists = spark.catalog.tableExists(silver_table)\n",
    "    if not table_exists:\n",
    "        if \"created_date\" not in df.columns:\n",
    "            df = df.withColumn(\"created_date\", current_timestamp())\n",
    "        if \"created_by\" not in df.columns:\n",
    "            df = df.withColumn(\"created_by\", lit(\"anoopdk\"))\n",
    "        df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "    else:\n",
    "        df.createOrReplaceTempView(\"staging_view\")\n",
    "        merge_condition = \" AND \".join([f\"t.{pk}=s.{pk}\" for pk in pk_fields])\n",
    "        set_clause = \", \".join([f\"t.{c}=s.{c}\" for c in df.columns])\n",
    "        insert_cols = \", \".join(df.columns)\n",
    "        insert_vals = \", \".join([f\"s.{c}\" for c in df.columns])\n",
    "        merge_sql = f\"\"\"\n",
    "            MERGE INTO {silver_table} AS t\n",
    "            USING staging_view AS s\n",
    "            ON {merge_condition}\n",
    "            WHEN MATCHED THEN UPDATE SET {set_clause}\n",
    "            WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\n",
    "        \"\"\"\n",
    "        spark.sql(merge_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83ddb90e-6219-4df4-b250-99b62b67503f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def incremental_upsert(spark, df, table_name, pk_fields):\n",
    "#     from pyspark.sql.functions import lit, current_timestamp\n",
    "\n",
    "#     silver_table = f\"workspace.silver.{table_name}\"\n",
    "#     table_exists = spark.catalog.tableExists(silver_table)\n",
    "\n",
    "#     # Add metadata columns if not present\n",
    "#     metadata_cols = [\"created_date\", \"created_by\", \"updated_date\", \"updated_by\"]\n",
    "#     for col_name in metadata_cols:\n",
    "#         if col_name not in df.columns:\n",
    "#             df = df.withColumn(col_name, lit(None).cast(\"timestamp\" if \"date\" in col_name else \"string\"))\n",
    "\n",
    "#     if not table_exists:\n",
    "#         # For initial ingestion, set created_date/current_timestamp, created_by/'anoopdk', updated_date/None, updated_by/None\n",
    "#         df = (\n",
    "#             df.withColumn(\"created_date\", current_timestamp())\n",
    "#               .withColumn(\"created_by\", lit(\"anoopdk\"))\n",
    "#               .withColumn(\"updated_date\", lit(None).cast(\"timestamp\"))\n",
    "#               .withColumn(\"updated_by\", lit(None).cast(\"string\"))\n",
    "#         )\n",
    "#         df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(silver_table)\n",
    "#     else:\n",
    "#         df.createOrReplaceTempView(\"staging_view\")\n",
    "#         merge_condition = \" AND \".join([f\"t.{pk}=s.{pk}\" for pk in pk_fields])\n",
    "\n",
    "#         # Columns for update and insert\n",
    "#         update_set = []\n",
    "#         for c in df.columns:\n",
    "#             if c == \"updated_date\":\n",
    "#                 update_set.append(f\"t.updated_date = current_timestamp()\")\n",
    "#             elif c == \"updated_by\":\n",
    "#                 update_set.append(f\"t.updated_by = 'anoopdk'\")\n",
    "#             elif c not in [\"created_date\", \"created_by\"]:\n",
    "#                 update_set.append(f\"t.{c} = s.{c}\")\n",
    "#         set_clause = \", \".join(update_set)\n",
    "\n",
    "#         insert_cols = \", \".join(df.columns)\n",
    "#         insert_vals = []\n",
    "#         for c in df.columns:\n",
    "#             if c == \"created_date\":\n",
    "#                 insert_vals.append(\"current_timestamp()\")\n",
    "#             elif c == \"created_by\":\n",
    "#                 insert_vals.append(\"'anoopdk'\")\n",
    "#             elif c in [\"updated_date\", \"updated_by\"]:\n",
    "#                 insert_vals.append(\"NULL\")\n",
    "#             else:\n",
    "#                 insert_vals.append(f\"s.{c}\")\n",
    "#         insert_vals_str = \", \".join(insert_vals)\n",
    "\n",
    "#         merge_sql = f\"\"\"\n",
    "#             MERGE INTO {silver_table} t\n",
    "#             USING staging_view s\n",
    "#             ON {merge_condition}\n",
    "#             WHEN MATCHED THEN UPDATE SET {set_clause}\n",
    "#             WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals_str})\n",
    "#         \"\"\"\n",
    "#         spark.sql(merge_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "948ed8e6-9a74-46a6-bd0a-d8efbf7928a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_table(spark, table_name, pk_fields, not_null_fields, na_fields,string_cols, numeric_cols, date_cols):\n",
    "    df = load_bronze_table(spark, table_name)\n",
    "    if pk_fields:\n",
    "        check_not_null(df, pk_fields)\n",
    "        check_no_duplicates(df, pk_fields)\n",
    "    if not_null_fields:\n",
    "        check_not_null(df, not_null_fields)\n",
    "    if na_fields:\n",
    "        df = fill_na_with_default(df, na_fields)\n",
    "    if string_cols:\n",
    "        df = clean_string_columns(df, string_cols)\n",
    "    if numeric_cols:\n",
    "        df = clean_numeric_columns(df, numeric_cols)\n",
    "    if date_cols:\n",
    "        df = clean_date_columns(df, date_cols)\n",
    "    incremental_upsert(spark, df, table_name, pk_fields)\n",
    "    # rename_columns_from_constraints(spark, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b271ec7b-3cef-4e7b-b816-2135581488c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "constraints_df = (\n",
    "    spark.table(\"workspace.control.enrich_table_constraints\")\n",
    "    .filter((col(\"table_name\") == table_name) & (col(\"is_active\") == True))\n",
    ")\n",
    "\n",
    "pk_fields = [row.field_name for row in constraints_df.filter(col(\"field_constraint\") == \"primary key\").collect()]\n",
    "not_null_fields = [row.field_name for row in constraints_df.filter(col(\"field_constraint\") == \"not null\").collect()]\n",
    "na_fields = [\n",
    "    (row.field_name, row.default_value, row.field_data_type)\n",
    "    for row in constraints_df.filter((col(\"field_constraint\") == \"na\") & (col(\"default_value\").isNotNull())).collect()\n",
    "]\n",
    "\n",
    "# table_name = \"transaction\"\n",
    "fields_df = (\n",
    "    spark.table(\"workspace.control.source_file_details\")\n",
    "    .filter(col(\"file_name\") == table_name)\n",
    "    .join(\n",
    "        spark.table(\"workspace.control.source_file_specifications\"),\n",
    "        \"source_id\"\n",
    "    )\n",
    "    .select(\"field_name\", \"data_type\")\n",
    ")\n",
    "\n",
    "# display(fields_df)\n",
    "\n",
    "string_cols = [row.field_name for row in fields_df.filter(col(\"data_type\").rlike(\"(?i)string|char|text\")).collect()]\n",
    "# display(string_cols)\n",
    "numeric_cols = [row.field_name for row in fields_df.filter(col(\"data_type\").rlike(\"(?i)int|float|double|decimal|number\")).collect()]\n",
    "# display(numeric_cols)\n",
    "date_cols = [row.field_name for row in fields_df.filter(col(\"data_type\").rlike(\"(?i)date|timestamp\")).collect()]\n",
    "# display(date_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdfadbd8-69ed-4532-a18d-5b3067bf18ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "process_table(\n",
    "    spark,\n",
    "    table_name=table_name,\n",
    "    pk_fields=pk_fields,\n",
    "    not_null_fields=not_null_fields,\n",
    "    na_fields=na_fields,\n",
    "    string_cols=string_cols,\n",
    "    numeric_cols=numeric_cols,\n",
    "    date_cols=date_cols\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ir_orchestration_raw_to_enrich",
   "widgets": {
    "table_name": {
     "currentValue": "investor",
     "nuid": "ec566caf-7e86-419e-82c9-bd2225545833",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "investor",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "investor",
      "label": null,
      "name": "table_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
